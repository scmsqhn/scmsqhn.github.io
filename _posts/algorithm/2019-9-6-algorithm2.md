---
layout: blog
title: "NLP算法四类问题应用简介"
---

使用BERT结构模型解决四类自然语言处理问题适用方法与场景及问题分析


## 1. 自然语言处理算法简述 algorithm of nlp
### nlp算法任务包括四个部分
#### a 句子级文本分类任务
- 应用于什么任务
  > 文本分类,

- 什么场景效果好
  > **在字词层面具有区别的领域文本分类**
  ```
  比如,足球,篮球属于体育
  素描,国画属于美术
  ```

- 什么场景近似却效果不好
  > **相同领域,但是具有相似的描述**
  ```
  比如,在银行柜台办理业务的流水记录,所使用词汇,过程均相似
  但是属于不同的类别,如,对公贷款,对私贷款,投诉,理赔等
  文本中蕴含有逻辑关系或推断的情况,效果有所下降
  ```

#### b 字符级别文本抽取任务
- 应用于什么任务
  > 人事地案物,实体名称提取

- 什么场景效果好
  > **上下文文本具有文法一致性**
  ```
  比如,踢足球,打篮球,打乒乓球
  爬山,爬长城,爬楼梯
  ```

- 什么场景近似却效果不好
  > **上下文有相对简单的逻辑和推断**
  ```
  比如,我在公交车上丢了一部手机,我以为是前面的小伙子偷得,\
    后来发现不是,又以为是后面的小姑娘偷得,最后发现梦醒了.
  ```

#### c 句子级别句子对关系判断

- 应用于什么任务

  > 蕴含关系判断

  ```
  比如: 我爱运动 我爱打篮球 1
  label:0 逻辑冲突 1 逻辑不冲突 2 待定
  ```

- 什么场景效果好
  ```
  **文本长度短**
  **文本句子对,表达充实,文法特征丰富**

  举例: 
  我吃着火锅,唱着歌 我唱着歌,吃着火锅 1
  连续三天,我中午都吃火锅  昨天我中午吃火锅 1
  ```

- 什么场景近似却效果不好
  **文本长度较长**
  **文本短单一**
  ```
  比如,
    [xxx...] 网路诈骗 1
    [xxx...] 网路诈骗 0
    [xxx...] 网路诈骗 0
    [xxx...] 网路诈骗 0
    [xxx...] 网路诈骗 0
    [xxx...] 网路诈骗 0
  或者,
    [xxx...] 网路诈骗 0
    [xxx...] 网路诈骗 1
    [xxx...] 网路诈骗 1
    [xxx...] 网路诈骗 1
    [xxx...] 网路诈骗 1
    [xxx...] 网路诈骗 1
  ```
  
#### d 字符级别文本抽取任务 及 文本级别生成任务
- 应用于什么任务
  ```
  阅读理解
  翻译
  机器写作
  ```

- 什么场景效果好
  ```
  答案唯一的阅读理解,基于简单逻辑的文本抽取
  文本,正文,文法关系丰富,逻辑简单清晰
  ```

- 什么场景近似却效果不好

  ```
  **其他场景效果都不理想,离工业化有一定距离**
  语音到语音的端到端翻译任务和阅读理解是,自然语言处理的终极任务
  ```

## 2. 我们的位置 where we are
  ```
  c- ==> c+ 
  nlp中小规模团队和企业，主要集中在此位置
  ```

## 3. 工具与手段 how we do this

  - bert + 规则

  - bert结构图

  ![BERT介绍](https://www.jianshu.com/p/d7ce41b58801)
 
  - 自注意力层

  ![自注意力层+RES-net](https://upload-images.jianshu.io/upload_images/14927967-2056c387bd0da3c3.png?imageMogr2/auto-orient/strip|imageView2/2/w/1112/format/webp)

  - 优缺点
  ```
  快: GPU
  准: all to all 不只是seq 2 seq
  顺序问题:  处理字符顺序的能力较弱，添加'位置层',所以中文较英文差
              中文 比英文低10% 汉藏语系 详见专利
  CNN层: 模型前后添加卷积更快的完成特征发现和训练,没有更准,佐证注意力层+ResNet(更深的网路结构)
          在发现文法特征上,有着目前所有结构最好的效果
  ```

  > 思考 逻辑特征 文法特征区别

  ```
  1. 从极端情况分析,短文本中的文法特征等于逻辑关系.
  2. 模型的目的是用统计规律(矩阵向量)表达文法关系与逻辑关系的映射.
  3. 符号不能更好的矫正逻辑,它本身就是逻辑,它只能帮助更好地理解.
  **逻辑关系与文法特征,在nlp处理，可以认为没区别**
  ```

## 4. 解决办法


  - 如何优化分类及实体抽取中的效果
    ```
    **涉及字符级逻辑关系推断的分类及抽取任务**
    分两步完成:
    1. 上下文文法一致的状态下，完成序列标注任务
    2. 使用句子对训练,解决蕴含关系推断
       使用句子对抽取,解决阅读理解任务
    3. 将两次结果，序贯处理后后输出
    run_squad.py      阅读理解
    run_classifier.py 逻辑蕴含推断
    ```


  - 样本问题
    ```
    处理，标注，训练大量的样本的问题，滞后解决，使用切割后的短样本进行训练
    比如
      早上天气晴朗,我去学校上学,走在路上有人抢我的包 拦路抢劫 1 
    转换成 
      早上天气晴朗 拦路抢劫 0 
      我去学校上学 拦路抢劫 0 
      走在路上有人抢我的包 拦路抢劫 1 

    比如
      早上天气晴朗,我去学校上学,走在路上有人抢我的包,是同学和我开玩笑 拦路抢劫 0 
    转换成 
      早上天气晴朗 拦路抢劫 0 
      我去学校上学 拦路抢劫 0 
      走在路上有人抢我的包,是同学和我开玩笑 拦路抢劫 0 

    **这类处理，需要增加正确的分句和分句后处理**

    更多更好更丰富的样本
    如何存储,管理更大量的文本
    如何处理更大数量的文本,逻辑蕴涵关系的训练数目都在50k-500k级别往上
        目前的硬件资源单次训练20轮需要10hrs+
    如何找出最准确的,最容易突破的点,形成成功经验进行复制.
    ```

  - 样本标注
    ```
    0 不确定 没有发生约定类别的行为
    1 是的 发生了约定类别的行为，并且是当前标签
    2 不是 发生了约定类别的行为，但是不是当前标签

    1 是的 发生了约定类别的行为，并且是当前标签
    0 不确定 不确定是否为1
    ```

  - WGAN尝试
    ```
    优点 1 生成样本
         2 在只有阳性样本的情况下，训练判别器
    缺点 1 速度慢
         2 准确率待验证
    待BERT模型输出后，再集中时间调测
    ```

## 6. 实验结果
  - 实验进行中









