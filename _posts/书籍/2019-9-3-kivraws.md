---
layout: post
title: 新词发现验证
---

齐夫定力 符号 新词发现



##  齐夫回归发现新词
- 使用图连通性
- 度
- '齐夫定理'用回归方法发现新词,令到发现的新词更加符合一般语言中的分布,更准确

---
# 专利正文部分
## 为什么人类用逻辑和符号进行思考和验算,是符号选择了人类,还是人类选择了符号.
## 无论是谁选择了谁,在使用,演化的过程中,一定会有些参数是像'自然对数','圆周率'等,因为符合效率和进化的要求,而固化下来,用回归工具去发现这些参数,是可以在现有的条件下更好的进行分词,断句,等固定文法环境下的自然语言处理初基础务.

# 本文将介绍一种使用奇夫定理 与 多源非线性回归方法进行新词发现的方法.

## 奇夫定理介绍

```
定律内容

如果将一篇较长的文章(约5000字以上)中每个词按其出现频次递减排列起来(高频词在前，低频词在后)，并用自然数给这些词编上等级序号，出现频次最高的为1级，其次为2级……这样一直到D级，如果用f表示词在文章中出现的频次，用r表示词的等级序号，则有:

fr=C(C为常数)

如果等级r与频次f都取对数，则图4-4中的双曲线变成一条直线(图4-5)。与之等价的数学表达式为:

ln(r)+ln(f)=ln(C)

这条直线由图4-5中实线表示。如果将这一方程改写可得:

kfr=C

如果说fr=C是齐夫定律的一般形式，这便是齐夫定律的修正形式。

这个"定律"是哈佛大学的语言学家GeorgeKingsley Zipf1949年发表的。比如，在 Brown 语料库中，"the"是最常见的单词，它在这个语料库中出现了大约7%(100万单词中出现69971次)。正如齐夫定律中所描述的一样，出现次数为第二位的单词"of"占了整个语料库中的3.5%(36411次)，之后的是"and"(28852次)。仅仅135个字汇就占了Brown语料库的一半。
累积分布函数累积分布函数
齐夫定律是一个实验定律，而非理论定律。齐夫分布可以在很多现象中被观察到。齐夫分布的在现实中的起因是一个争论的焦点。齐夫定律很容易用点阵图观察，坐标为log(排名)和log(频率)。比如，"the"用上述表述可以描述为x = log(1), y = log(69971)的点。如果所有的点接近一条直线，那么它就遵循齐夫定律。最简单的齐夫定律的例子是"1/f function"。给出一组齐夫分布的频率，按照从最常见到非常见排列，第二常见的频率是最常见频率的出现次数的&frac12;，第三常见的频率是最常见的频率的1/3，第n常见的频率是最常见频率出现次数的1/n。然而，这并不精确，因为所有的项必须出现一个整数次数，一个单词不可能出现2.5次。然而，在一个广域范围内并且做出适当的近似，许多自然现象都符合齐夫定律。

```
## 总结:  index=n, num=total/n, 取log,作图,接近线性
```
我们假定语言是有生命的,他的生命就是传播和交流,用更高的效率进行传播和交流,这就需要使用更少的符号,因为符号意味着成本的错误,意味着更多理解分歧,或者模糊.字是符号,词是符号,标点也是符号,我们假定,语言的演化方向是更少的词,更通俗的表达,听说读写的一致.
这里我们选择是否趋同于奇夫定律 和 是否使用了更少的词,来进行判断,我们的分词是否正确.
以上,便是本文的核心思想

下面,我们的代码也将使用log作为转化尺度,便于线性可分
```
# 流程图

```flow
st=>start: 新词发现
in=>inputoutput: 将一定规模的文本>5000
op1=>operation: 数据清洗
op2=>operation: 生成有向无环图
op3=>operation: 对图上结点求熵与互熵(是一种改进的互熵)
op4=>yes: 可能是一个词组
op5=>no: 不可能是一个词组
cond1=>condition: 将熵与互熵按照一定阈值进行过滤
cond1(yes)=>
cond=>condition: Yes or No?
EMdis=>operation: 使用Wasserstein距离
sigmod=>end: 归一化输出sigmod
output=>end: 输出最终分类
st->vec
vec->bert
bert->charvec
charvec->cdnn
cdnn->point
point(yes)->EMdis
point(no)->sigmod
EMdis->output
sigmod->output
```


